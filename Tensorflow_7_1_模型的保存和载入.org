* Tensorflow model save and load

#+BEGIN_SRC ipython :tangle yes :noweb yes :session lec2-simple-MNIST :exports code :async t :results raw drawer
  <<包导入>>
  # <<导入 projector: for embeddings 可视化>>


  <<数据准备>>
  # numpy构造(with/without noise)
  # 数据集位置
  # 数据集导入内存(one_hot or not)
  # 截取部分数据集

  <<图参数>>
  # 批次大小, batch_size
  # 批次数量, n_batch
  # dropout 保留率, keep_prob
  # + <<for rnn >>n_inputs: dimension of each item(a vector) of sequence
  # + <<for rnn >>max_time: the max length of all sequences(maybe the size of Dataset), max_time also means the iteration time of rnn layer
  # + <<for rnn >>lstm_size: inside of each rnn layer, how many lstm units
  # + <<for rnn >>n_classes: units of ful nn layer

  <<工具函数与工具声明>>
  # 对某些 Variable 进行 OP 并 summary
  # <<def Variable: for embeddings 可视化>> as untrainable Variable, stack front 3000 img, give name 'embeddings'
  # <<file IO: for embeddings 可视化>> read in one_hot labels, argmax get true labels, write to file in one-label-one-line format
  # W, b 初始化工具函数

  <<图构造>>
  # 一神: NN layers, name_scope for TB, 参数 summary
  #   1. Placeholders
  #      1.1 x: dataset placeholder,
  #      + <<def OP: for img process, CNN[-1, height, width, channels], RNN[-1, max_time, n_inputs] >> reshape x  ------+
  #      1.2 y: labelset placeholder,                                                                                   |
  #      1.3 keep_prob: dropout, keep rate of certain layer's nodes                                                     |
  #   2. Layers & Variables                                                                                             |
  #      2.0 名称空间设置                                                                                               |
  #      2.1 第一层权重 W,                  声明 summary tf.summary.scalar/image/histogram node                         |
  #      2.2 第一层偏置 b,                  声明 summary tf.summary.scalar/image/histogram node                         |
  #      2.3 第一层输出(active_fn(logits)), 声明 summary tf.summary.scalar/image/histogram node                         |
  #      + <<conv2d layer: for CNN>> 只接受 [batch_size, height, width, channels] 格式 <--------------------------------+
  #      + <<max_pool layer: for CNN>>                                                                                  |
  #      + <<BasicLSTMCell: for RNN>>                                                                                   |
  #      + <<dynamic_rnn(units, inputs): for RNN>>                                                                      |
  #                               ^                                                                                     |
  #                               +-------------------------------------------------------------------------------------+

  # 两函:
  #   1. err_fn:
  #      1.1 名称空间设置
  #      1.2 err fn(单点错误), 声明 summary, tf.summary.scalar/image/histogram node
  #   2. loss_fn:
  #      2.1 名称空间设置
  #      2.2 loss fn(整体错误), 声明 summary, tf.summary.scalar/image/histogram node

  # 三器:
  #   1. 初始化器
  #   2. 优化器
  #      2.1 名称空间设置
  #   3. 保存器

  <<图构造善后>>
  # 准确率
  #   1. correct_prediction
  #      1.1 名称空间设置
  #   2. accuracy
  #      2.1 名称空间设置
  # 合并 summary
  # + <<for embeddings 可视化>>配置 embeddings 可视化参数

  <<图计算>>
  # 运行初始化器
  # summary Writer for TB
  # for epoch_num: <<
  #          1. for batch_num:
  #                 1.1 x_y_of_next_batch;
  #                 1.2 运行 优化器计算 and summary计算
  #          2. 运行准确率计算
  # 运行保存器
  # matplot绘图
#+END_SRC


#+BEGIN_SRC ipython -n :tangle yes :session lec7-save lec1 :exports code :async t :results raw drawer
  import tensorflow as tf
  from tensorflow.examples.tutorials.mnist import input_data

  # 载入数据
  mnist = input_data.read_data_sets("MNIST", one_hot=True)

  # 设置批次大小
  batch_size = 100
  # 计算共有多少批次
  n_batch = mnist.train.num_examples // batch_size

  # 定义两个 placeholder
  x = tf.placeholder(tf.float32, [None, 784])
  y = tf.placeholder(tf.float32, [None, 10])

  # 创建简单神经网络(无隐藏层)
  W = tf.Variable(tf.zeros([784, 10]))
  b = tf.Variable(tf.zeros([10]))
  prediction = tf.nn.softmax(tf.matmul(x, W) + b)

  # 二函,二器
  init = tf.global_variables_initializer()
  optimizer = tf.train.GradientDescentOptimizer(0.2)
  loss = tf.reduce_mean(tf.square(y-prediction))
  train = optimizer.minimize(loss)

  # 预测对错存在一个向量中
  correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(prediction, 1))
  # 计算准确率
  accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))


  saver = tf.train.Saver()


  # 图计算
  with tf.Session() as sess:
      sess.run(init)
      # 采取训练一轮就测试一轮的方式
      for epoch in range(21):
          # 训练模型
          acc_train = 0
          for batch in range(n_batch):
              batch_xs, batch_ys = mnist.train.next_batch(batch_size)
              _, acc_train = sess.run([train, accuracy], feed_dict={x:batch_xs, y:batch_ys})

          # 测试模型
          # 测试集必须使用已经训练完毕的模型
          acc_test = sess.run(accuracy, feed_dict={x:mnist.test.images, y:mnist.test.labels})
          print("Iter " + str(epoch) + " ,Train:" + str(acc_train) + " ,Test:" + str(acc_test))

      # 保存模型
      # 注意代码缩进, 他很明显是训练完成后的代码, 保存的是 session
      saver.save(sess, 'net/my_net.ckpt')
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[1]:
:END:

上面的代码会在原本为空的 net/ 文件夹下产生如下四个文件:

#+BEGIN_EXAMPLE
  -rw-r--r--  1 yiddi yiddi   79 7月  31 03:09 checkpoint
  -rw-r--r--  1 yiddi yiddi  31K 7月  31 03:09 my_net.ckpt.data-00000-of-00001
  -rw-r--r--  1 yiddi yiddi  159 7月  31 03:09 my_net.ckpt.index
  -rw-r--r--  1 yiddi yiddi  16K 7月  31 03:09 my_net.ckpt.meta
#+END_EXAMPLE


#+BEGIN_SRC ipython -n :tangle yes :session lec7-load lec1 :exports code :async t :results raw drawer
  import tensorflow as tf
  from tensorflow.examples.tutorials.mnist import input_data

  # 载入数据
  mnist = input_data.read_data_sets("MNIST", one_hot=True) (ref:one_hot)

  # 设置批次大小
  batch_size = 100                                         (ref:batch_size)
  # 计算共有多少批次
  n_batch = mnist.train.num_examples // batch_size         (ref:floor division)

  # 定义两个 placeholder
  x = tf.placeholder(tf.float32, [None, 784])
  y = tf.placeholder(tf.float32, [None, 10])

  # 创建简单神经网络(无隐藏层)
  W = tf.Variable(tf.zeros([784, 10]))
  b = tf.Variable(tf.zeros([10]))
  prediction = tf.nn.softmax(tf.matmul(x, W) + b)

  # 二函,二器
  init = tf.global_variables_initializer()
  optimizer = tf.train.GradientDescentOptimizer(0.2)
  loss = tf.reduce_mean(tf.square(y-prediction))
  train = optimizer.minimize(loss)

  # 预测对错存在一个向量中
  correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(prediction, 1)) (ref:count correct prediction)
  # 计算准确率
  accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))


  saver = tf.train.Saver()

  ####################################################################
  # 在此之前, 也就是图构建过程与之前的程序完全一样
  ####################################################################

  # 图计算
  with tf.Session() as sess:
      sess.run(init)

      print(sess.run(accuracy, feed_dict={x:mnist.test.images, y:mnist.test.labels}))
      saver.restore(sess, 'net/my_net.ckpt')
      print(sess.run(accuracy, feed_dict={x:mnist.test.images, y:mnist.test.labels}))
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[1]:
0.098
INFO:tensorflow:Restoring parameters from net/my_net.ckpt
0.9137
:END:
